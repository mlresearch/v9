---
title: Infinite Predictor Subspace Models for Multitask Learning
abstract: Given several related learning tasks, we propose a nonparametric Bayesian
  model that captures task relatedness by assuming that the task parameters (i.e.,
  predictors) share a latent subspace. More specifically, the intrinsic dimensionality
  of the task subspace is not assumed to be known a priori. We use an infinite latent
  feature model to automatically infer this number (depending on and limited by only
  the number of tasks). Furthermore, our approach is applicable when the underlying
  task parameter subspace is inherently sparse, drawing parallels with l1 regularization
  and LASSO-style models. We also propose an augmented model which can make use of
  (labeled, and additionally unlabeled if available) inputs to assist learning this
  subspace, leading to further improvements in the performance. Experimental results
  demonstrate the efficacy of both the proposed approaches, especially when the number
  of examples per task is small. Finally, we discuss an extension of the proposed
  framework where a nonparametric mixture of linear subspaces can be used to learn
  a manifold over the task parameters, and also deal with the issue of negative transfer
  from unrelated tasks.
pdf: http://proceedings.mlr.press/v9/rai10a/rai10a.pdf
layout: inproceedings
series: Proceedings of Machine Learning Research
id: rai10a
month: 0
firstpage: '613'
lastpage: '620'
page: 613-620
sections: 
author:
- given: Piyush
  family: Rai
- given: Hal Daume
  family: III
date: 2010-03-31
address: Chia Laguna Resort, Sardinia, Italy
publisher: PMLR
container-title: Proceedings of the Thirteenth International Conference on Artificial
  Intelligence and Statistics
volume: '9'
genre: inproceedings
issued:
  date-parts:
  - 2010
  - 3
  - 31
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
